{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e4bf58-770b-4eb6-a48f-b581aa7db7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7ccac2-9783-4535-ae5e-0b23ba918957",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'BATCH_SIZE': 2**11,\n",
    "    'EPOCHS': 100,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'SEED' : 42,\n",
    "    'MODEL_NAME' : 'gdcn_ecn_fm_wll_full.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da7dbb4-afa4-4138-8826-7907ef279dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b8beab-3175-426b-859b-27d195880431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10704179, 119)\n",
      "Test shape: (1527298, 118)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "all_train = pd.read_parquet(\"./train.parquet\", engine=\"pyarrow\")\n",
    "test = pd.read_parquet(\"./test.parquet\", engine=\"pyarrow\").drop(columns=['ID'])\n",
    "\n",
    "print(\"Train shape:\", all_train.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99d05a8-98b0-4244-9c0a-a9f54d8d14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature 분리\n",
    "cat_cols = {}\n",
    "num_cols = []\n",
    "for col in all_train.columns:\n",
    "    if col == 'clicked' or col == 'seq':\n",
    "        continue\n",
    "    if (all_train[col].astype(float).fillna(0) % 1 == 0).all():  # 소수점 이하가 전부 0인지 체크 - int 분류\n",
    "        l = len(all_train[col].dropna().unique())\n",
    "        if l < 100: # category column 분류\n",
    "            cat_cols[col] = int(all_train[col].astype(float).max())\n",
    "        else:\n",
    "            num_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad56558-ed66-493e-9cde-47747efd37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicked == 1 데이터\n",
    "clicked_1 = all_train[all_train['clicked'] == 1]\n",
    "\n",
    "# clicked == 0 데이터에서 동일 개수x2 만큼 무작위 추출 (다운 샘플링)\n",
    "clicked_0 = all_train[all_train['clicked'] == 0].sample(n=len(clicked_1)*9, random_state=42)\n",
    "\n",
    "# 두 데이터프레임 합치기\n",
    "train = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4614e1d-38f5-4b35-a251-55caf62a6268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2041790, 119)\n",
      "Train clicked:0: (1837611, 119)\n",
      "Train clicked:1: (204179, 119)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Train clicked:0:\", train[train['clicked']==0].shape)\n",
    "print(\"Train clicked:1:\", train[train['clicked']==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127060d-199d-48ca-bf48-c18806d94cf7",
   "metadata": {},
   "source": [
    "# Data Column Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d72c6e44-4f5e-466c-acc6-3e2d5a3c108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 117\n",
      "Num categorical features: 23\n",
      "Num numerical features: 94\n",
      "Sequence: seq\n",
      "Target: clicked\n"
     ]
    }
   ],
   "source": [
    "# Target / Sequence\n",
    "target_col = \"clicked\"\n",
    "seq_col = \"seq\"\n",
    "\n",
    "# 학습에 사용할 피처: ID/seq/target 제외, 나머지 전부\n",
    "FEATURE_EXCLUDE = {target_col, seq_col, \"ID\"}\n",
    "feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]\n",
    "\n",
    "print(\"Num features:\", len(feature_cols))\n",
    "print(\"Num categorical features:\", len(cat_cols))\n",
    "print(\"Num numerical features:\", len(num_cols))\n",
    "print(\"Sequence:\", seq_col)\n",
    "print(\"Target:\", target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190aa8ba-f375-47fb-95f7-69a3bfdf2430",
   "metadata": {},
   "source": [
    "# Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835ec174-94ba-4d04-9086-7296bae2437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, seq_col, target_col=None, has_target=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cat_cols = list(cat_cols.keys())\n",
    "        self.num_cols = num_cols\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "\n",
    "        # 비-시퀀스 피처: category - int, num - float\n",
    "        self.cats = self.df[self.cat_cols].astype(float).fillna(0).astype(int).values\n",
    "        self.nums = self.df[self.num_cols].astype(float).fillna(0).values\n",
    "\n",
    "        # 시퀀스: 문자열 그대로 보관 (lazy 파싱)\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
    "\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cats = torch.tensor(self.cats[idx], dtype=torch.int)\n",
    "        nums = torch.tensor(self.nums[idx], dtype=torch.float)\n",
    "\n",
    "        # 전체 시퀀스 사용 (빈 시퀀스만 방어)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array([], dtype=np.float32)\n",
    "\n",
    "        if arr.size == 0:\n",
    "            arr = np.array([0.0], dtype=np.float32)  # 빈 시퀀스 방어\n",
    "\n",
    "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
    "\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return cats, nums, seq, y\n",
    "        else:\n",
    "            return cats, nums, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05dcd34f-16f3-4690-9c40-127a244b20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch):\n",
    "    cats, nums, seqs, ys = zip(*batch)\n",
    "    cats = torch.stack(cats)\n",
    "    nums = torch.stack(nums)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)  # 빈 시퀀스 방지\n",
    "    return cats, nums, seqs_padded, seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    cats, nums, seqs = zip(*batch)\n",
    "    cats = torch.stack(cats)\n",
    "    nums = torch.stack(nums)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return cats, nums, seqs_padded, seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c23d3e-27d7-4837-9c62-98979cb92d05",
   "metadata": {},
   "source": [
    "# Define Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40503e3-6009-47c7-a1c8-70f1d642dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialCrossNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.Dropout(0.2)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for l, layer in enumerate(self.layers, start=1):\n",
    "            mask = torch.sigmoid(layer(out))  # Self-Mask 기능\n",
    "            out = x.pow(2**(l-1)) * mask\n",
    "        return out\n",
    "        \n",
    "class DNNLyaer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "        \n",
    "class FMLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        square_of_sum = torch.pow(x.sum(dim=1), 2)\n",
    "        sum_of_square = (x * x).sum(dim=1)\n",
    "        return (0.5 * (square_of_sum - sum_of_square)).unsqueeze(1)  # [batch_size, 1]\n",
    "        \n",
    "class GatedCrossLayerBlock(nn.Module):\n",
    "    def __init__(self, input_dim : int, num_layers : int, gate_function = nn.Sigmoid()):\n",
    "        # input_dim : embedding_vector_dim * number of feature?\n",
    "        super(GatedCrossLayerBlock, self).__init__()\n",
    "        input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gate_function = gate_function\n",
    "        \n",
    "        self.wc = nn.ModuleList() # weight of cross layer\n",
    "        self.wg = nn.ModuleList() # weight of gate layer\n",
    "        self.bias = nn.ParameterList() # bias\n",
    "        for _ in range(self.num_layers):\n",
    "            self.wc.append(\n",
    "                nn.Sequential(nn.Linear(input_dim, input_dim), nn.Dropout(0.2))\n",
    "            )\n",
    "            self.wg.append(\n",
    "                nn.Sequential(nn.Linear(input_dim, input_dim), nn.Dropout(0.2))\n",
    "            )\n",
    "            self.bias.append(nn.Parameter(torch.zeros(input_dim)))\n",
    "            \n",
    "        for m in self.wg.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        # cross networking\n",
    "        '''\n",
    "        c_(l+1) = c0*(wc_l + bias_l) * gated(wg_l) + c_l\n",
    "        \n",
    "        wc_l : nn.linear(c_l)\n",
    "        wg_l : nn.linear(c_l)\n",
    "        '''\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            xc = self.wc[i](x)\n",
    "            xg = self.gate_function(self.wg[i](x))\n",
    "\n",
    "            x = x0*(xc+self.bias[i])*xg + x\n",
    "\n",
    "        return x # [batch_size, input_dim]\n",
    "\n",
    "        \n",
    "class TabularSeqModel(nn.Module):\n",
    "    def __init__(self, cat_cols, num_cols, seq_emb_dim=32, feature_emb_dim = 8, num_layers = 4, mlp_hidden_units=[1024, 512, 256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        # categorical feature part\n",
    "        n_cats = len(cat_cols)\n",
    "        self.cat_embs = nn.ModuleList([\n",
    "            nn.Embedding(num_categories+1, feature_emb_dim, padding_idx=0)\n",
    "            for num_categories in cat_cols.values()\n",
    "        ])\n",
    "        \n",
    "        # numerical feature part\n",
    "        n_cols = len(num_cols)\n",
    "        self.num_proj = nn.BatchNorm1d(n_cols)#ProjectionLayer(n_cols, feature_emb_dim)\n",
    "\n",
    "        ## sequantial layer block\n",
    "        # seq: 숫자 시퀀스 → LSTM\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=seq_emb_dim, batch_first=True)\n",
    "\n",
    "        ## feature train layer blocks\n",
    "        # FM Layer Block - 저차원\n",
    "        fm_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim\n",
    "        fm_output_dim = 1\n",
    "        self.fm = FMLayer()\n",
    "        print(f'mdel fm_dim : {fm_input_dim}, output_dim : {fm_output_dim}')\n",
    "\n",
    "        # Gated Cross Layer Block - 저차원 선택적 학습\n",
    "        gcn_output_dim = gcn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim\n",
    "        self.gcn = GatedCrossLayerBlock(input_dim=gcn_input_dim, num_layers=num_layers)\n",
    "        print('mdel gcn_dim : ', gcn_input_dim)\n",
    "\n",
    "        # Exponential Cross Layer Block - 고차원 선택적 학습\n",
    "        ecn_output_dim = ecn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim\n",
    "        self.ecn = ExponentialCrossNetwork(input_dim = ecn_input_dim, num_layers=min(3,num_layers)) # 크면 터짐\n",
    "        print('mdel ecn_dim : ', ecn_input_dim)\n",
    "\n",
    "        # DNN Layer Block - 고차원\n",
    "        dnn_output_dim = dnn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim\n",
    "        self.dnn = DNNLyaer(dnn_input_dim, [dnn_input_dim for _ in range(num_layers)])\n",
    "        print('mdel dnn_dim : ', dnn_input_dim)\n",
    "        \n",
    "        ## 최종 MLP\n",
    "        final_input_dim = fm_output_dim + gcn_output_dim + ecn_output_dim + dnn_output_dim # \n",
    "        print('mdel final_input_dim : ', final_input_dim)\n",
    "        layers = []\n",
    "        for h in mlp_hidden_units:\n",
    "            linear = nn.Linear(final_input_dim, h)\n",
    "            nn.init.kaiming_normal_(linear.weight, nonlinearity='relu')\n",
    "            #nn.init.xavier_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers += [linear, nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            final_input_dim = h\n",
    "\n",
    "        final_linear = nn.Linear(final_input_dim, 1)\n",
    "        nn.init.xavier_normal_(final_linear.weight)\n",
    "        nn.init.zeros_(final_linear.bias)\n",
    "        \n",
    "        layers += [final_linear, nn.Sigmoid()]\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cats, x_nums, x_seq, seq_lengths):\n",
    "        # categorical feature part\n",
    "        embedded_cats = [\n",
    "            emb(x_cats[:, i])  # shape: (batch_size, embedding_dim)\n",
    "            for i, emb in enumerate(self.cat_embs)\n",
    "        ]\n",
    "        embedded_cats = torch.cat(embedded_cats, dim=1) # (B, n_cats*emb_dim)\n",
    "\n",
    "        # numerical feature part\n",
    "        embedded_nums = self.num_proj(x_nums) # (B, n_nums), BatchNormalize\n",
    "\n",
    "        # concat\n",
    "        x = torch.cat([embedded_cats, embedded_nums], dim=1) # (B, n_cats*feature_emb_dim + bn_cols)\n",
    "        \n",
    "        # 시퀀스 → LSTM (pack)\n",
    "        x_seq = x_seq.unsqueeze(-1)  # (B, L, 1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = h_n[-1]    # (B,emb_dim)\n",
    "\n",
    "        z = torch.cat([x, h], dim=1) # (B, n_cats*feature_emb_dim + bn_cols + seq_emb_dim)\n",
    "        \n",
    "        # FM LayerBlock\n",
    "        z_fm = self.fm(z)\n",
    "        # GC LayerBlock\n",
    "        z_gc = self.gcn(z)\n",
    "        # EC LayerBlock\n",
    "        z_ec = self.ecn(z)\n",
    "        # DNN LayerBLock\n",
    "        z_dnn = self.dnn(z)\n",
    "        \n",
    "        z = torch.cat([z_fm\n",
    "                       ,z_gc\n",
    "                       ,z_ec\n",
    "                       ,z_dnn], dim=1)\n",
    "        \n",
    "        return self.mlp(z).squeeze(1)  # logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb408dd-ef88-40a5-8d9c-bf2f9ae35e18",
   "metadata": {},
   "source": [
    "# Train / Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b14f783-d5f6-4f05-9333-02e873a0095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weight = [1,1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = weight[0]/sum(weight)\n",
    "        self.w1 = weight[1]/sum(weight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.float()\n",
    "\n",
    "        N0 = (y_true == 0).sum()\n",
    "        N1 = (y_true == 1).sum()\n",
    "\n",
    "        w0 = self.w0 / N0\n",
    "        w1 = self.w1 / N1\n",
    "\n",
    "        sample_weights = torch.where(y_true == 0, w0, w1)\n",
    "        bce_loss = F.binary_cross_entropy(y_pred, y_true, reduction='none')\n",
    "        weighted_loss = (bce_loss * sample_weights).sum()\n",
    "        \n",
    "        return weighted_loss\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self\n",
    "                 , model = None\n",
    "                 , loss_func = nn.BCELoss()\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def fit(self\n",
    "            , train_df\n",
    "            , feature_cols : list\n",
    "            , seq_col : str\n",
    "            , target_col : str\n",
    "            , batch_size : int = 512\n",
    "            , epochs : int = 3\n",
    "            , learning_rate : float = 1e-3\n",
    "            , device : str = None\n",
    "            , model_name : str = 'model.pth'\n",
    "           ):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        #self.device = 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        print(f'training with {self.device}')\n",
    "        # 1) split\n",
    "        tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "        # 2) Dataset / Loader (l_max 인자 제거)\n",
    "        train_dataset = ClickDataset(tr_df, cat_cols, num_cols, seq_col, target_col, has_target=True)\n",
    "        eval_dataset   = ClickDataset(va_df, cat_cols, num_cols, seq_col, target_col, has_target=True)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn_train)\n",
    "        eval_loader   = DataLoader(eval_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn_train)\n",
    "\n",
    "        # 2-1) optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=1)\n",
    "        \n",
    "        # 3) Loop\n",
    "        self.last_loss = 0\n",
    "        early_stop_count = 0\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train_loss = self._batch_process(train_loader, is_train = True)\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                eval_loss = self._batch_process(eval_loader, is_train = False)\n",
    "            \n",
    "            current_lr = self.scheduler.get_last_lr()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {eval_loss:.4f} | Learning Rate : {current_lr}\")\n",
    "            if self.last_loss < eval_loss:\n",
    "                self.last_loss = eval_loss\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                if early_stop_count >= 3:\n",
    "                    break\n",
    "\n",
    "        print(f'last score : {self.last_loss}')\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def _batch_process(self, loader, is_train : bool = True):\n",
    "        if is_train:\n",
    "            self.model.train()\n",
    "            mode = 'Train'\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            mode = 'Eval'\n",
    "\n",
    "        losses = [] # eval mode에서 preds 겸용\n",
    "        nums = [] # eval mode에서 trues 겸용\n",
    "        for xcats, xnums, seqs, seq_lens, ys in tqdm(loader, desc=mode):\n",
    "            xcats, xnums, seqs, seq_lens, ys = xcats.to(self.device), xnums.to(self.device), seqs.to(self.device), seq_lens.to(self.device), ys.to(self.device)\n",
    "            logits = self.model(xcats, xnums, seqs, seq_lens)\n",
    "            if is_train:\n",
    "                loss, num = self._get_loss(logits, ys, is_train)\n",
    "                losses.append(loss)\n",
    "                nums.append(num)\n",
    "            else:\n",
    "                losses.append(logits.to('cpu').detach())\n",
    "                nums.append(ys.to('cpu').detach())\n",
    "\n",
    "        if is_train:\n",
    "            total_loss = sum([x*y for x,y in zip(losses, nums)])\n",
    "            total_loss = total_loss/sum(nums)\n",
    "        else:\n",
    "            all_preds = torch.cat(losses).view(-1)\n",
    "            all_trues = torch.cat(nums).view(-1)\n",
    "\n",
    "            total_loss = 0.5 * average_precision_score(all_trues.numpy(), all_preds.numpy()) + 0.5 * 1/(1+WeightedBCELoss()(all_preds, all_trues).item())\n",
    "\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def _get_loss(self, preds, labels, is_train : bool):\n",
    "        \n",
    "        loss = self.loss_func(preds,labels)\n",
    "        if is_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return loss.item(), preds.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05da1f-dbf5-4687-af09-9f7d0b85bc90",
   "metadata": {},
   "source": [
    "# Run!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c6345-4276-4b45-be93-d9efc7b66cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdel fm_dim : 250, output_dim : 1\n",
      "mdel gcn_dim :  250\n",
      "mdel ecn_dim :  250\n",
      "mdel dnn_dim :  250\n",
      "mdel final_input_dim :  751\n",
      "{'BATCH_SIZE': 2048, 'EPOCHS': 100, 'LEARNING_RATE': 0.001, 'SEED': 42, 'MODEL_NAME': 'gdcn_ecn_fm_wll_full.pth'}\n",
      "training with cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   5%|▍         | 36/798 [00:15<05:39,  2.25it/s]"
     ]
    }
   ],
   "source": [
    "# 3) 모델\n",
    "model_args = {'cat_cols' : cat_cols\n",
    "              , 'num_cols' : num_cols\n",
    "              , 'seq_emb_dim' : 64\n",
    "              , 'feature_emb_dim' : 4\n",
    "              , 'num_layers' : 5\n",
    "              , 'mlp_hidden_units' : [512,128]\n",
    "              , 'dropout' : 0.2}\n",
    "\n",
    "model = TabularSeqModel(**model_args)\n",
    "\n",
    "criterion = WeightedBCELoss()\n",
    "\n",
    "trainer = Trainer(model, loss_func = criterion)\n",
    "\n",
    "\n",
    "print(CFG)\n",
    "trainer.fit(\n",
    "    train_df=train,\n",
    "    feature_cols=feature_cols,\n",
    "    seq_col=seq_col,\n",
    "    target_col=target_col,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    epochs=CFG['EPOCHS'],\n",
    "    learning_rate=CFG['LEARNING_RATE'],\n",
    "    model_name = CFG['MODEL_NAME']\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58a6f2-c006-4ada-a615-211cb8adf50a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b19298-c51a-4cea-8401-a0342a6fb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Dataset/Loader\n",
    "test_ds = ClickDataset(test, cat_cols, num_cols, seq_col, target_col, has_target=False)\n",
    "test_ld = DataLoader(test_ds, batch_size=CFG['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn_infer)\n",
    "\n",
    "# 2) Predict\n",
    "model = TabularSeqModel(**model_args)\n",
    "model.load_state_dict(torch.load(CFG['MODEL_NAME']))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for xcats, xnums, seqs, seq_lens in tqdm(test_ld, desc=\"Inference\"):\n",
    "        xcats, xnums, seqs, seq_lens = xcats.to(device), xnums.to(device), seqs.to(device), seq_lens.to(device)\n",
    "        outs.append(model(xcats, xnums, seqs, seq_lens).cpu())\n",
    "\n",
    "test_preds = torch.cat(outs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14e0d4-89e8-4731-bdeb-6e348143f77d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3cd9b8-8596-4770-8544-2ec39991fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['clicked'] = test_preds\n",
    "\n",
    "file_name = CFG['MODEL_NAME'].split('.')[0]\n",
    "submit.to_csv(f'./{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6383881-73bd-43f1-8acb-f10a8d8ab24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'./{file_name}.csv')['clicked'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
