{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e4bf58-770b-4eb6-a48f-b581aa7db7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7ccac2-9783-4535-ae5e-0b23ba918957",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'BATCH_SIZE': 2**12\n",
    "    , 'EPOCHS': 40\n",
    "    , 'LEARNING_RATE': 1e-3\n",
    "    , 'SEED' : 42\n",
    "    , 'MAX_SEQ_LEN' : 2**9\n",
    "    , 'MODEL_NAME' : 'lstm_leakyrelu.pth'\n",
    "    , 'DOWN_SAMPLE_RATE' : 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da7dbb4-afa4-4138-8826-7907ef279dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127060d-199d-48ca-bf48-c18806d94cf7",
   "metadata": {},
   "source": [
    "# Data Pre-processing for Train, Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033455cb-d54b-4e85-9a0b-23655e927a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    def __init__(self\n",
    "                 , path : str = '.'\n",
    "                 , seed : int = 42\n",
    "                 , down_sample_rate : int = 2\n",
    "                ):\n",
    "        self.path = path\n",
    "        self.seed = seed\n",
    "        self.down_sample_rate = down_sample_rate\n",
    "\n",
    "        # Target / Sequence\n",
    "        self.target_col = \"clicked\"\n",
    "        self.ads_col = 'l_feat_14'\n",
    "        self.seq_col = \"seq\"\n",
    "\n",
    "    def data_pre_process(self):\n",
    "\n",
    "        train, test = self._load_data()\n",
    "\n",
    "        train, test, cat_cols, le_dict, num_cols = self._category_sep(train, test)\n",
    "\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.cat_cols = cat_cols\n",
    "        self.le_dict = le_dict\n",
    "        self.num_cols = num_cols\n",
    "        self.max_seq_col = 1000#self.extract_unique_values_batch(self.train[self.seq_col]).astype(int).max()\n",
    "\n",
    "        self.le_ads_col, self.n_ads_col = self._ads_col_label_encoding()\n",
    "\n",
    "        self.train = self.cat_to_label(self.train)\n",
    "        self.train[self.ads_col] = self._get_each_label(self.le_ads_col, self.train[self.ads_col])\n",
    "        \n",
    "        self.test = self.cat_to_label(self.test)\n",
    "        self.test[self.ads_col] = self._get_each_label(self.le_ads_col, self.test[self.ads_col])\n",
    "\n",
    "        self.add_engineered_feature(is_train_df = True)\n",
    "        self.add_engineered_feature(is_train_df = False)\n",
    "        \n",
    "    def _load_data(self):\n",
    "        # 데이터 로드\n",
    "        all_train = pd.read_parquet(self.path + \"/train.parquet\", engine=\"pyarrow\")\n",
    "        test = pd.read_parquet(self.path + \"/test.parquet\", engine=\"pyarrow\").drop(columns=['ID'])\n",
    "        \n",
    "        print(\"Train shape:\", all_train.shape)\n",
    "        print(\"Test shape:\", test.shape)\n",
    "        \n",
    "        return self._down_sampling(all_train), test #all_train, test#\n",
    "\n",
    "    def _category_sep(self, train, test):\n",
    "        # categorical feature 분리\n",
    "        cat_cols = {}\n",
    "        le_dict = {}\n",
    "        num_cols = []\n",
    "        for col in train.columns:\n",
    "            if col == self.target_col or col == self.ads_col or col == self.seq_col:\n",
    "                continue\n",
    "            if (train[col].astype(float).fillna(0) % 1 == 0).all():  # 소수점 이하가 전부 0인지 체크 - int 분류\n",
    "                l = len(train[col].dropna().unique())\n",
    "                #print(col, l)\n",
    "                \n",
    "                if l < 100: # category column 분류\n",
    "                    le_dict[col] = LabelEncoder()\n",
    "                    le_dict[col].fit(train[col].dropna()) # 결측치는 Torch DataSet 처리 : 일단 빼기, category 수만 체크\n",
    "                    cat_cols[col] = len(le_dict[col].classes_)\n",
    "                else:\n",
    "                    num_cols.append(col)\n",
    "                    \n",
    "            else:\n",
    "                num_cols.append(col)\n",
    "\n",
    "        return train, test, cat_cols, le_dict, num_cols\n",
    "\n",
    "    def _down_sampling(self, train):\n",
    "        # clicked == 1 데이터\n",
    "        clicked_1 = train[train['clicked'] == 1]\n",
    "        \n",
    "        # clicked == 0 데이터에서 동일 개수x2 만큼 무작위 추출 (다운 샘플링)\n",
    "        clicked_0 = train[train['clicked'] == 0].sample(n=len(clicked_1)*self.down_sample_rate, random_state=self.seed)\n",
    "        \n",
    "        # 두 데이터프레임 합치기\n",
    "        train = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
    "\n",
    "        print(\"Train shape:\", train.shape)\n",
    "        print(\"Train clicked:0:\", train[train['clicked']==0].shape)\n",
    "        print(\"Train clicked:1:\", train[train['clicked']==1].shape)\n",
    "\n",
    "        return train\n",
    "\n",
    "    def _ads_col_label_encoding(self):\n",
    "        le_ads_col = LabelEncoder()\n",
    "        le_ads_col.fit(self.train[self.ads_col].dropna())\n",
    "        n_ads_col = len(le_ads_col.classes_)\n",
    "        return le_ads_col, n_ads_col\n",
    "\n",
    "    def cat_to_label(self, data):\n",
    "        for col in tqdm(self.le_dict.keys(),  desc='Label Encoding'):\n",
    "            data[col] = self._get_each_label(self.le_dict[col], data[col])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def add_engineered_feature(self, is_train_df = True):\n",
    "        print(f'get engineered feature : is_train = {is_train_df}')\n",
    "        if is_train_df:\n",
    "            data = self.train\n",
    "        else:\n",
    "            data = self.test\n",
    "        tqdm.pandas()\n",
    "        data[['seq_len', 'seq_unique', 'seq_diversity']] = data['seq'].progress_apply(\n",
    "            lambda x: pd.Series(self.get_seq_stats(x))\n",
    "        )\n",
    "\n",
    "\n",
    "        if is_train_df:\n",
    "            self.train = data\n",
    "            for col in ['seq_len', 'seq_unique', 'seq_diversity']:\n",
    "                self.num_cols.append(col)\n",
    "        else:\n",
    "            self.test = data\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def _get_each_label(le, data_list):\n",
    "        n = data_list.shape\n",
    "        label2id = {label: idx + 1 for idx, label in enumerate(le.classes_)}\n",
    "        data_list = pd.Series(data_list)\n",
    "        result = data_list.map(label2id).to_numpy()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    #### seq col 처리를 위한 함수\n",
    "    @staticmethod\n",
    "    def extract_unique_values(batch):\n",
    "        # 배치 내 고유값 추출\n",
    "        unique_values = (\n",
    "            batch.dropna()\n",
    "                 .str.split(',')\n",
    "                 .explode()\n",
    "                 .unique()\n",
    "        )\n",
    "        return unique_values\n",
    "    \n",
    "    def extract_unique_values_batch(self, series, batch_size=100000):\n",
    "        all_uniques = set()\n",
    "        \n",
    "        for start in tqdm(range(0, len(series), batch_size), desc='Extract Unique Values from Sequence Columns'):\n",
    "            end = start + batch_size\n",
    "            batch = series[start:end]\n",
    "            unique_values = self.extract_unique_values(batch)\n",
    "            all_uniques.update(unique_values)\n",
    "    \n",
    "        # 최종 유니크한 값 정렬된 리스트로 반환\n",
    "        return np.array(sorted(all_uniques))\n",
    "    @staticmethod\n",
    "    def get_seq_stats(seq_str):\n",
    "        if not seq_str:\n",
    "            return 0.0, 0.0, 0.0\n",
    "        tokens = seq_str.split(',')\n",
    "        length = len(tokens)\n",
    "        unique_cnt = len(set(tokens))\n",
    "        diversity = unique_cnt / length if length > 0 else 0.0\n",
    "        return length/100, unique_cnt/10, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299e7fd9-229d-4638-86c4-05c48c419eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage(path = '.'\n",
    "                  , seed = CFG['SEED']\n",
    "                  , down_sample_rate = CFG['DOWN_SAMPLE_RATE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9415a25-92d0-4bcc-86b7-11a84b6d63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10704179, 119)\n",
      "Test shape: (1527298, 118)\n",
      "Train shape: (2041790, 119)\n",
      "Train clicked:0: (1837611, 119)\n",
      "Train clicked:1: (204179, 119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Label Encoding: 100%|██████████| 23/23 [00:01<00:00, 22.07it/s]\n",
      "Label Encoding: 100%|██████████| 23/23 [00:00<00:00, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get engineered feature : is_train = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2041790/2041790 [03:56<00:00, 8647.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get engineered feature : is_train = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1527298/1527298 [02:53<00:00, 8798.13it/s] \n"
     ]
    }
   ],
   "source": [
    "storage.data_pre_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72c6e44-4f5e-466c-acc6-3e2d5a3c108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num categorical features: 23\n",
      "Num numerical features: 96\n",
      "Target Ads: l_feat_14\n",
      "Sequence: seq\n",
      "Target: clicked\n"
     ]
    }
   ],
   "source": [
    "# Target / Sequence\n",
    "\n",
    "print(\"Num categorical features:\", len(storage.cat_cols))\n",
    "print(\"Num numerical features:\", len(storage.num_cols))\n",
    "print(\"Target Ads:\", storage.ads_col)\n",
    "print(\"Sequence:\", storage.seq_col)\n",
    "print(\"Target:\", storage.target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190aa8ba-f375-47fb-95f7-69a3bfdf2430",
   "metadata": {},
   "source": [
    "# Define Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835ec174-94ba-4d04-9086-7296bae2437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, cat_cols, num_cols, ads_col, seq_col, max_seq_len, target_col=None, has_target=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.cat_cols = list(cat_cols.keys())\n",
    "        self.num_cols = num_cols\n",
    "        self.ads_col = ads_col\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # 비-시퀀스 피처: category - int, num - float\n",
    "        self.cats = self.df[self.cat_cols].astype(float).fillna(0).astype(int).values\n",
    "        self.nums = self.df[self.num_cols].astype(float).fillna(0).values\n",
    "        self.ads = self.df[self.ads_col].astype(float).fillna(0).astype(int).values\n",
    "\n",
    "        # 시퀀스: 문자열 그대로 보관 (lazy 파싱)\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
    "\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cats = torch.tensor(self.cats[idx], dtype=torch.int)\n",
    "        nums = torch.tensor(self.nums[idx], dtype=torch.float)\n",
    "        ads = torch.tensor(self.ads[idx], dtype=torch.int).unsqueeze(dim=0)\n",
    "\n",
    "        # 전체 시퀀스 사용 (빈 시퀀스만 방어)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            ## seq len 제한\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)[-self.max_seq_len:] # 2000개 초과 비율 약 0.5%, 클릭율 1.8%로 전체(1.9%)보다 낮음\n",
    "        else:\n",
    "            arr = np.array([], dtype=np.float32)\n",
    "\n",
    "        if arr.size == 0:\n",
    "            arr = np.array([0.0], dtype=np.float32)  # 빈 시퀀스 방어\n",
    "\n",
    "        seq = torch.from_numpy(arr)  # shape (seq_len,)\n",
    "\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return cats, nums, ads, seq, y\n",
    "        else:\n",
    "            return cats, nums, ads, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05dcd34f-16f3-4690-9c40-127a244b20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch):\n",
    "    cats, nums, ads, seqs, ys = zip(*batch)\n",
    "    cats = torch.stack(cats)\n",
    "    nums = torch.stack(nums)\n",
    "    ads = torch.stack(ads)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)  # 빈 시퀀스 방지\n",
    "    return cats, nums, ads, seqs_padded.to(torch.int32), seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    cats, nums, ads, seqs = zip(*batch)\n",
    "    cats = torch.stack(cats)\n",
    "    nums = torch.stack(nums)\n",
    "    ads = torch.stack(ads)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return cats, nums, ads, seqs_padded.to(torch.int32), seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c23d3e-27d7-4837-9c62-98979cb92d05",
   "metadata": {},
   "source": [
    "# Define Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40503e3-6009-47c7-a1c8-70f1d642dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # [T, D]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [T, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [D/2]\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd\n",
    "        pe = pe.unsqueeze(0)  # [1, T, D]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, D]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads=4, num_layers=2, dim_feedforward=128, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, max_len=max_len)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Important: [B, T, D]\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        x: [B, T, D]\n",
    "        key_padding_mask: [B, T] with True for PAD positions\n",
    "        \"\"\"\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding_mask)  # [B, T, D]\n",
    "        return x\n",
    "\n",
    "class TargetAdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.attn_score = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, seq_out, target, mask=None):\n",
    "        \"\"\"\n",
    "        seq_out: [B, T, H]\n",
    "        target: [B, H]\n",
    "        mask: [B, T] (optional) - True for valid positions\n",
    "        \"\"\"\n",
    "        B, T, H = seq_out.size()\n",
    "        \n",
    "        # Expand target embedding to match sequence length\n",
    "        query_expanded = target.unsqueeze(1).expand(-1, T, -1)  # [B, T, H]\n",
    "        \n",
    "        # Concatenate LSTM output and target embedding\n",
    "        concat = torch.cat([seq_out, query_expanded], dim=-1)  # [B, T, 2H]\n",
    "        \n",
    "        # Feed through MLP to get attention score\n",
    "        attn_hidden = torch.tanh(self.attn_layer(concat))   # [B, T, H]\n",
    "        attn_hidden = self.dropout(attn_hidden)  \n",
    "        scores = self.attn_score(attn_hidden).squeeze(-1)   # [B, T]\n",
    "        \n",
    "        # Masking (optional)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))  # mask False = pad\n",
    "\n",
    "        # Attention weights\n",
    "        weights = F.softmax(scores, dim=1)  # [B, T]\n",
    "\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.bmm(weights.unsqueeze(1), seq_out).squeeze(1)  # [B, H]\n",
    "        \n",
    "        return context, weights\n",
    "        \n",
    "class GRULayerBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, dropout=0.0, batch_first=True, max_seq_len=2**10):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.gru = nn.GRU(input_size=input_size\n",
    "                          , hidden_size=hidden_size\n",
    "                          , num_layers=num_layers\n",
    "                          , bidirectional=bidirectional\n",
    "                          , dropout=dropout\n",
    "                          , batch_first=batch_first\n",
    "                         )\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, h_n = self.gru(packed)\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=self.max_seq_len)  # [B, T, H]\n",
    "        else:\n",
    "            output, h_n = self.gru(x)  # no packing\n",
    "\n",
    "        # c_n은 LSTM에서만 존재하므로 None으로 리턴\n",
    "        return output, (h_n, None)\n",
    "\n",
    "class LSTMLayerBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False, dropout=0.0, batch_first=True, max_seq_len=2**10):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.lstm = nn.LSTM(input_size=input_size\n",
    "                            , hidden_size=hidden_size\n",
    "                            , num_layers=num_layers\n",
    "                            , bidirectional=bidirectional\n",
    "                            , dropout=dropout\n",
    "                            , batch_first=batch_first\n",
    "                           )\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        if lengths is not None:\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            packed_out, (h_n, c_n) = self.lstm(packed)\n",
    "            output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=self.max_seq_len)  # [B, T, H]\n",
    "        else:\n",
    "            output, (h_n, c_n) = self.lstm(x)  # no packing\n",
    "            \n",
    "        return output, (h_n, c_n)\n",
    "        \n",
    "class ExponentialCrossNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=2, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for l, layer in enumerate(self.layers, start=1):\n",
    "            mask = torch.sigmoid(layer(out))  # Self-Mask 기능\n",
    "            out = x.pow(2**(l-1)) * mask\n",
    "        return out\n",
    "        \n",
    "class GatedCrossLayerBlock(nn.Module):\n",
    "    def __init__(self\n",
    "                 , input_dim : int\n",
    "                 , num_layers : int\n",
    "                 , dropout : float = 0.2\n",
    "                 , gate_function = nn.Sigmoid()\n",
    "                ):\n",
    "        # input_dim : embedding_vector_dim * number of feature?\n",
    "        super(GatedCrossLayerBlock, self).__init__()\n",
    "        input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gate_function = gate_function\n",
    "        \n",
    "        self.wc = nn.ModuleList() # weight of cross layer\n",
    "        self.wg = nn.ModuleList() # weight of gate layer\n",
    "        self.bias = nn.ParameterList() # bias\n",
    "        for _ in range(self.num_layers):\n",
    "            self.wc.append(\n",
    "                nn.Sequential(nn.Linear(input_dim, input_dim), nn.Dropout(dropout))\n",
    "            )\n",
    "            self.wg.append(\n",
    "                nn.Sequential(nn.Linear(input_dim, input_dim), nn.Dropout(dropout))\n",
    "            )\n",
    "            self.bias.append(nn.Parameter(torch.zeros(input_dim)))\n",
    "            \n",
    "        for m in self.wg.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        # cross networking\n",
    "        '''\n",
    "        c_(l+1) = c0*(wc_l + bias_l) * gated(wg_l) + c_l\n",
    "        \n",
    "        wc_l : nn.linear(c_l)\n",
    "        wg_l : nn.linear(c_l)\n",
    "        '''\n",
    "        x0 = x\n",
    "        for i in range(self.num_layers):\n",
    "            xc = self.wc[i](x)\n",
    "            xg = self.gate_function(self.wg[i](x))\n",
    "\n",
    "            x = x0*(xc+self.bias[i])*xg + x\n",
    "\n",
    "        return x # [batch_size, input_dim]\n",
    "        \n",
    "class DNNLyaer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        nag_slope = 0.01\n",
    "        for h in hidden_dims:\n",
    "            linear = nn.Linear(input_dim, h)\n",
    "            nn.init.kaiming_normal_(linear.weight, a=nag_slope, nonlinearity='leaky_relu')\n",
    "            nn.init.zeros_(linear.bias)\n",
    "        \n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=nag_slope))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            input_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim*3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([x\n",
    "                       , x**2\n",
    "                       #, torch.log(torch.clamp(x,min=1e-6))\n",
    "                       , x**3\n",
    "                      ], dim=1)\n",
    "        return self.linear(x)  # (B, 1)\n",
    "        \n",
    "class FMLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        square_of_sum = torch.pow(x.sum(dim=1), 2)\n",
    "        sum_of_square = (x * x).sum(dim=1)\n",
    "        return (0.5 * (square_of_sum - sum_of_square)).unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        \n",
    "class TabularSeqModel(nn.Module):\n",
    "    def __init__(self\n",
    "                 , cat_cols\n",
    "                 , num_cols\n",
    "                 , n_ads_col\n",
    "                 , max_seq_col\n",
    "                 , max_seq_len\n",
    "                 , seq_emb_dim=32\n",
    "                 , feature_emb_dim = 8\n",
    "                 , num_layers = 4\n",
    "                 , mlp_hidden_units=[1024, 512, 256, 128]\n",
    "                 , dropout=0.2\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # categorical feature part\n",
    "        n_cats = len(cat_cols)\n",
    "        self.cat_embs = nn.ModuleList([\n",
    "            nn.Embedding(num_categories+1, feature_emb_dim, padding_idx=0)\n",
    "            for num_categories in cat_cols.values()\n",
    "        ])\n",
    "        \n",
    "        # numerical feature part\n",
    "        n_cols = len(num_cols)\n",
    "        self.num_proj = nn.BatchNorm1d(n_cols)#ProjectionLayer(n_cols, feature_emb_dim)\n",
    "\n",
    "        # ads column part\n",
    "        self.ads_emb = nn.Embedding(n_ads_col+1, seq_emb_dim, padding_idx = 0)\n",
    "\n",
    "        ## sequantial layer block\n",
    "        # seq: 숫자 시퀀스 → LSTM\n",
    "        self.seq_emb = nn.Embedding(max_seq_col+1, seq_emb_dim, padding_idx=0)\n",
    "        self.seq_module = LSTMLayerBlock(input_size=seq_emb_dim\n",
    "                                         , hidden_size=seq_emb_dim\n",
    "                                         , num_layers=min(2,num_layers)\n",
    "                                         , batch_first=True\n",
    "                                         , dropout=min(0.3,dropout)\n",
    "                                         , max_seq_len=max_seq_len\n",
    "                                        )#GRULayerBlock\n",
    "\n",
    "        # attention with ads set\n",
    "        attn_output_dim = seq_emb_dim\n",
    "        self.attn = TargetAdditiveAttention(hidden_dim = seq_emb_dim, dropout=min(0.2,dropout))\n",
    "\n",
    "        ## feature train layer blocks\n",
    "        # Wide Layer Block - 1차원\n",
    "        wd_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim + seq_emb_dim + seq_emb_dim\n",
    "        wd_output_dim = 1\n",
    "        self.wide = Wide(input_dim=wd_input_dim)\n",
    "        print(f'model wide_dim : {wd_input_dim}, output_dim : {wd_output_dim}')\n",
    "        \n",
    "        # FM Layer Block - 저차원\n",
    "        fm_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim + seq_emb_dim + seq_emb_dim\n",
    "        fm_output_dim = 1\n",
    "        self.fm = FMLayer()\n",
    "        print(f'model fm_dim : {fm_input_dim}, output_dim : {fm_output_dim}')\n",
    "\n",
    "        # Gated Cross Layer Block - 저차원 선택적 학습\n",
    "        gcn_output_dim = gcn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim + seq_emb_dim + seq_emb_dim # ads_col, seq_col, attn 3개\n",
    "        self.gcn = GatedCrossLayerBlock(input_dim=gcn_input_dim, num_layers=num_layers, dropout = dropout)\n",
    "        print(f'model gcn_dim : {gcn_input_dim}')\n",
    "\n",
    "        # Exponential Cross Layer Block - 고차원 선택적 학습\n",
    "        ecn_output_dim = ecn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim + seq_emb_dim + seq_emb_dim\n",
    "        self.ecn = ExponentialCrossNetwork(input_dim = ecn_input_dim, num_layers=min(3,num_layers), dropout = dropout) # 크면 터짐\n",
    "        print(f'model ecn_dim : {ecn_input_dim}')\n",
    "\n",
    "        # DNN Layer Block - 고차원\n",
    "        dnn_output_dim = dnn_input_dim = n_cats*feature_emb_dim + n_cols + seq_emb_dim + seq_emb_dim + seq_emb_dim\n",
    "        self.dnn = DNNLyaer(input_dim = dnn_input_dim, hidden_dims = [dnn_input_dim for _ in range(num_layers)], dropout = dropout)\n",
    "        print(f'model dnn_dim : {dnn_input_dim}')\n",
    "        \n",
    "        ## 최종 MLP\n",
    "        final_input_dim = wd_output_dim + fm_output_dim + gcn_output_dim + ecn_output_dim + dnn_output_dim # \n",
    "        print(f'model final_input_dim : {final_input_dim}')\n",
    "        layers = []\n",
    "        nag_slope = 0.1\n",
    "        for h in mlp_hidden_units:\n",
    "            linear = nn.Linear(final_input_dim, h)\n",
    "            nn.init.kaiming_normal_(linear.weight, a=nag_slope, nonlinearity='leaky_relu')\n",
    "            #nn.init.xavier_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers += [linear, nn.BatchNorm1d(h), nn.LeakyReLU(negative_slope=nag_slope), nn.Dropout(dropout)]\n",
    "            final_input_dim = h\n",
    "\n",
    "        final_linear = nn.Linear(final_input_dim, 1)\n",
    "        nn.init.xavier_normal_(final_linear.weight)\n",
    "        nn.init.zeros_(final_linear.bias)\n",
    "        \n",
    "        layers += [final_linear, nn.Sigmoid()]\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cats, x_nums, x_ads, x_seq, seq_lengths):\n",
    "        # categorical feature part\n",
    "        embedded_cats = [\n",
    "            emb(x_cats[:, i])  # shape: (batch_size, embedding_dim)\n",
    "            for i, emb in enumerate(self.cat_embs)\n",
    "        ]\n",
    "        embedded_cats = torch.cat(embedded_cats, dim=1) # (B, n_cats*emb_dim)\n",
    "\n",
    "        # numerical feature part\n",
    "        embedded_nums = self.num_proj(x_nums) # (B, n_nums), BatchNormalize\n",
    "\n",
    "        # ads set part\n",
    "        embedded_ads = self.ads_emb(x_ads)\n",
    "        embedded_ads = embedded_ads.squeeze(1)\n",
    "\n",
    "        # concat\n",
    "        x = torch.cat([embedded_cats, embedded_nums, embedded_ads], dim=1) # (B, n_cats*feature_emb_dim + bn_cols)\n",
    "        \n",
    "        # 시퀀스 → GRU\n",
    "        x_seq = self.seq_emb(x_seq)  # (B, L, seq_emb)\n",
    "        seq_out, (h_n, c_n) = self.seq_module(x_seq, seq_lengths)\n",
    "        h = h_n[-1]    # (B,emb_dim)\n",
    "        mask = torch.arange(seq_out.size(1)).unsqueeze(0).to(self.device) < seq_lengths.unsqueeze(1)\n",
    "        attn_output, attn_weights = self.attn(seq_out, embedded_ads, mask=mask)\n",
    "        \n",
    "\n",
    "        z = torch.cat([x, h, attn_output], dim=1) # (B, n_cats*feature_emb_dim + bn_cols + seq_emb_dim)\n",
    "\n",
    "        # Wide LayerBlock\n",
    "        z_wd = self.wide(z)\n",
    "        # FM LayerBlock\n",
    "        z_fm = self.fm(z)\n",
    "        # GC LayerBlock\n",
    "        z_gc = self.gcn(z)\n",
    "        # EC LayerBlock\n",
    "        z_ec = self.ecn(z)\n",
    "        # DNN LayerBLock\n",
    "        z_dnn = self.dnn(z)\n",
    "        \n",
    "        z = torch.cat([z_wd\n",
    "                       ,z_fm\n",
    "                       ,z_gc\n",
    "                       ,z_ec\n",
    "                       ,z_dnn], dim=1)\n",
    "        \n",
    "        return self.mlp(z).squeeze(1)  # logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb408dd-ef88-40a5-8d9c-bf2f9ae35e18",
   "metadata": {},
   "source": [
    "# Train / Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b14f783-d5f6-4f05-9333-02e873a0095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weight = [1,1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = weight[0]/sum(weight)\n",
    "        self.w1 = weight[1]/sum(weight)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.float()\n",
    "        y_pred = y_pred.float()\n",
    "\n",
    "        N0 = (y_true == 0).sum()\n",
    "        N1 = (y_true == 1).sum()\n",
    "\n",
    "        w0 = self.w0 / N0\n",
    "        w1 = self.w1 / N1\n",
    "\n",
    "        sample_weights = torch.where(y_true == 0, w0, w1)\n",
    "        bce_loss = F.binary_cross_entropy(y_pred, y_true, reduction='none')\n",
    "        weighted_loss = (bce_loss * sample_weights).sum()\n",
    "        \n",
    "        return weighted_loss\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self\n",
    "                 , model = None\n",
    "                 , loss_func = nn.BCELoss()\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "    def fit(self\n",
    "            , train_df\n",
    "            , cat_cols : dict\n",
    "            , num_cols : list\n",
    "            , ads_col : str\n",
    "            , seq_col : str\n",
    "            , max_seq_len : int\n",
    "            , target_col : str\n",
    "            , batch_size : int = 512\n",
    "            , epochs : int = 3\n",
    "            , learning_rate : float = 1e-3\n",
    "            , device : str = None\n",
    "            , model_name : str = 'model.pth'\n",
    "           ):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        #self.device = 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        print(f'training with {self.device}')\n",
    "        # 1) split\n",
    "        tr_df, va_df = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "        # 2) Dataset / Loader\n",
    "        train_dataset = ClickDataset(tr_df, cat_cols, num_cols, ads_col, seq_col, max_seq_len, target_col, has_target=True)\n",
    "        eval_dataset   = ClickDataset(va_df, cat_cols, num_cols, ads_col, seq_col, max_seq_len, target_col, has_target=True)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn_train)\n",
    "        eval_loader   = DataLoader(eval_dataset, batch_size=batch_size//4, shuffle=False, collate_fn=collate_fn_train)\n",
    "\n",
    "        # 2-1) optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=epochs, T_mult=1)\n",
    "        \n",
    "        # 3) Loop\n",
    "        self.last_loss = 0\n",
    "        early_stop_count = 0\n",
    "        for epoch in range(1, epochs+1):\n",
    "            train_loss = self._batch_process(train_loader, is_train = True)\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                eval_loss = self._batch_process(eval_loader, is_train = False)\n",
    "            \n",
    "            current_lr = self.scheduler.get_last_lr()\n",
    "            \n",
    "            if self.last_loss < eval_loss:\n",
    "                early_stop_count = 0\n",
    "                self.last_loss = eval_loss\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "            else:\n",
    "                early_stop_count += 1\n",
    "                    \n",
    "            print(f\"[Epoch {epoch}] Train Loss: {train_loss:.6f} | Val Loss: {eval_loss:.6f} | Learning Rate : {current_lr} | stopping_count : {early_stop_count}\")\n",
    "            if early_stop_count >= 3:\n",
    "                break\n",
    "\n",
    "        print(f'last score : {self.last_loss}')\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def _batch_process(self, loader, is_train : bool = True):\n",
    "        if is_train:\n",
    "            self.model.train()\n",
    "            mode = 'Train'\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            mode = 'Eval'\n",
    "        torch.cuda.empty_cache()\n",
    "        losses = [] # eval mode에서 preds 겸용\n",
    "        nums = [] # eval mode에서 trues 겸용\n",
    "        for xcats, xnums, ads, seqs, seq_lens, ys in tqdm(loader, desc=mode):\n",
    "            xcats, xnums, ads, seqs, seq_lens, ys = xcats.to(self.device), xnums.to(self.device), ads.to(self.device), seqs.to(self.device), seq_lens.to(self.device), ys.to(self.device)\n",
    "            logits = self.model(xcats, xnums, ads, seqs, seq_lens)\n",
    "            if is_train:\n",
    "                loss, num = self._get_loss(logits, ys, is_train)\n",
    "                losses.append(loss)\n",
    "                nums.append(num)\n",
    "            else:\n",
    "                losses.append(logits.detach().cpu())\n",
    "                nums.append(ys.detach().cpu())\n",
    "\n",
    "        if is_train:\n",
    "            total_loss = sum([x*y for x,y in zip(losses, nums)])\n",
    "            total_loss = total_loss/sum(nums)\n",
    "        else:\n",
    "            all_preds = torch.cat(losses).view(-1)\n",
    "            all_trues = torch.cat(nums).view(-1)\n",
    "\n",
    "            total_loss = 0.5 * average_precision_score(all_trues.numpy(), all_preds.numpy()) + 0.5 * 1/(1+WeightedBCELoss()(all_preds, all_trues).item())\n",
    "\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def _get_loss(self, preds, labels, is_train : bool):\n",
    "        \n",
    "        loss = self.loss_func(preds,labels)\n",
    "        if is_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "        return loss.item(), preds.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05da1f-dbf5-4687-af09-9f7d0b85bc90",
   "metadata": {},
   "source": [
    "# Run!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d45c6345-4276-4b45-be93-d9efc7b66cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model wide_dim : 376, output_dim : 1\n",
      "model fm_dim : 376, output_dim : 1\n",
      "model gcn_dim : 376\n",
      "model ecn_dim : 376\n",
      "model dnn_dim : 376\n",
      "model final_input_dim : 1130\n",
      "{'BATCH_SIZE': 4096, 'EPOCHS': 40, 'LEARNING_RATE': 0.001, 'SEED': 42, 'MAX_SEQ_LEN': 512, 'MODEL_NAME': 'lstm_leakyrelu.pth', 'DOWN_SAMPLE_RATE': 9}\n",
      "training with cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:49<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.677461 | Val Loss: 0.437038 | Learning Rate : [1.541333133436018e-06] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:54<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.635286 | Val Loss: 0.441971 | Learning Rate : [6.15582970243117e-06] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.6224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.622421 | Val Loss: 0.446954 | Learning Rate : [1.3815039801161721e-05] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:53<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.6152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.615182 | Val Loss: 0.449763 | Learning Rate : [2.4471741852423235e-05] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:53<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.6106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.610611 | Val Loss: 0.450587 | Learning Rate : [3.806023374435663e-05] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.606361 | Val Loss: 0.452663 | Learning Rate : [5.449673790581611e-05] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:54<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.6043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.604307 | Val Loss: 0.452310 | Learning Rate : [7.367991782295391e-05] | stopping_count : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:54<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.6024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.602415 | Val Loss: 0.453695 | Learning Rate : [9.549150281252633e-05] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:54<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.600875 | Val Loss: 0.454729 | Learning Rate : [0.00011979701719998454] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.599189 | Val Loss: 0.454779 | Learning Rate : [0.00014644660940672628] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 0.596868 | Val Loss: 0.455555 | Learning Rate : [0.00017527597583490823] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.5954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 0.595372 | Val Loss: 0.456421 | Learning Rate : [0.00020610737385376348] | stopping_count : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:55<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 0.593429 | Val Loss: 0.456282 | Learning Rate : [0.00023875071764202561] | stopping_count : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:54<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.5923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 0.592334 | Val Loss: 0.454066 | Learning Rate : [0.00027300475013022663] | stopping_count : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 399/399 [22:52<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.5905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 399/399 [00:57<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 0.590530 | Val Loss: 0.455716 | Learning Rate : [0.0003086582838174551] | stopping_count : 3\n",
      "last score : 0.4564208939349199\n"
     ]
    }
   ],
   "source": [
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "\n",
    "# 3) 모델\n",
    "model_args = {'cat_cols' : storage.cat_cols\n",
    "              , 'num_cols' : storage.num_cols\n",
    "              , 'n_ads_col' : storage.n_ads_col\n",
    "              , 'max_seq_col' : storage.max_seq_col\n",
    "              , 'max_seq_len' : CFG['MAX_SEQ_LEN']\n",
    "              , 'seq_emb_dim' : 2**5\n",
    "              , 'feature_emb_dim' : 2**3\n",
    "              , 'num_layers' : 5\n",
    "              , 'mlp_hidden_units' : [512,128]\n",
    "              , 'dropout' : 0.4}\n",
    "\n",
    "model = TabularSeqModel(**model_args)\n",
    "\n",
    "criterion = WeightedBCELoss()\n",
    "\n",
    "trainer = Trainer(model, loss_func = criterion)\n",
    "\n",
    "\n",
    "print(CFG)\n",
    "trainer.fit(\n",
    "    train_df=storage.train,\n",
    "    cat_cols = storage.cat_cols,\n",
    "    num_cols = storage.num_cols,\n",
    "    ads_col = storage.ads_col,\n",
    "    seq_col = storage.seq_col,\n",
    "    max_seq_len = CFG['MAX_SEQ_LEN'],\n",
    "    target_col=storage.target_col,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    epochs=CFG['EPOCHS'],\n",
    "    learning_rate=CFG['LEARNING_RATE'],\n",
    "    model_name = CFG['MODEL_NAME']\n",
    ")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58a6f2-c006-4ada-a615-211cb8adf50a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5b19298-c51a-4cea-8401-a0342a6fb5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model wide_dim : 376, output_dim : 1\n",
      "model fm_dim : 376, output_dim : 1\n",
      "model gcn_dim : 376\n",
      "model ecn_dim : 376\n",
      "model dnn_dim : 376\n",
      "model final_input_dim : 1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1492/1492 [03:26<00:00,  7.22it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Dataset/Loader\n",
    "test_ds = ClickDataset(storage.test, storage.cat_cols, storage.num_cols, storage.ads_col, storage.seq_col, CFG['MAX_SEQ_LEN'], storage.target_col, has_target=False)\n",
    "test_ld = DataLoader(test_ds, batch_size=CFG['BATCH_SIZE']//4, shuffle=False, collate_fn=collate_fn_infer)\n",
    "\n",
    "# 2) Predict\n",
    "model = TabularSeqModel(**model_args)\n",
    "model.load_state_dict(torch.load(CFG['MODEL_NAME']))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "outs = []\n",
    "with torch.no_grad():\n",
    "    for xcats, xnums, ads, seqs, seq_lens in tqdm(test_ld, desc=\"Inference\"):\n",
    "        xcats, xnums, ads, seqs, seq_lens = xcats.to(device), xnums.to(device), ads.to(device), seqs.to(device), seq_lens.to(device)\n",
    "        outs.append(model(xcats, xnums, ads, seqs, seq_lens).cpu())\n",
    "\n",
    "test_preds = torch.cat(outs).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14e0d4-89e8-4731-bdeb-6e348143f77d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3cd9b8-8596-4770-8544-2ec39991fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['clicked'] = test_preds\n",
    "\n",
    "file_name = CFG['MODEL_NAME'].split('.')[0]\n",
    "submit.to_csv(f'./{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6383881-73bd-43f1-8acb-f10a8d8ab24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_csv(f'./{file_name}.csv')['clicked'].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
